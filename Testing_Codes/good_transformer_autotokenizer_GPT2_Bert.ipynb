{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f330c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your prompt: jjj\n",
      "{\n",
      "  \"id\": \"chatcmpl-7dDeuTb7i7TYs9lsyPt0vwv6CgGmF\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1689581652,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"I'm sorry, but I need a statement to convert to standard English.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 31,\n",
      "    \"completion_tokens\": 15,\n",
      "    \"total_tokens\": 46\n",
      "  }\n",
      "}\n",
      "Input is 'jjj'' and it used: 2 tokens\n",
      "Response is : 'I'm sorry, but I need a statement to convert to standard English.' and it used: 15 tokens calculated via GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)\n",
      "The response used 15 tokens calculated via response.usage.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "text_input = input('Please input your prompt: ')\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with statements, and your task is to convert them to standard English.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": text_input\n",
    "    }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=64,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "print (response)\n",
    "# Load the tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# The text to count tokens for\n",
    "text_response = response.choices[0].message[\"content\"]\n",
    "\n",
    "# Encode the text to count tokens\n",
    "encoded_response = tokenizer.encode(text_response)\n",
    "encoded_input = tokenizer.encode(text_input)\n",
    "\n",
    "# Count the number of tokens\n",
    "num_tokens_input = len(encoded_input)\n",
    "num_tokens_response = len(encoded_response)\n",
    "\n",
    "print(f\"Input is '{text_input}'' and it used: {num_tokens_input} tokens\")\n",
    "print(f\"Response is : '{text_response}' and it used: {num_tokens_response} tokens calculated via {tokenizer}\")\n",
    "print (f'The response used {response.usage[\"completion_tokens\"]} tokens calculated via response.usage.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c4eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
