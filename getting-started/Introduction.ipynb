{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "##### <a href=\"https://colab.research.google.com/github/SamurAIGPT/langchain-course/blob/main/getting-started/Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w12Jz3h_INct"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "**LangChain** is a software development framework designed to simplify the creation of applications that use large language models (LLMs) like OpenAI's GPT. It's written in Python and JavaScript, and it was launched as an open-source project in October 2022 by Harrison Chase, a developer working at the machine learning startup Robust Intelligence.\n",
    "\n",
    "The framework offers a broad range of capabilities and features for developers, allowing them to harness the power of LLMs for a variety of applications. Here are some key features of LangChain:\n",
    "\n",
    "**Ease of Use**: LangChain simplifies the process of integrating LLMs into applications. This makes it easier for developers to create and deploy applications that leverage language models for a variety of tasks, including document analysis, text summarization, and chatbot functionality.\n",
    "\n",
    "**Flexibility**: LangChain includes integrations with a variety of systems and services, including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; and support for multiple web scraping subsystems and templates.\n",
    "\n",
    "**Chain Mechanism:** At the heart of LangChain is the \"Chain\" mechanism, which is a sequence of operations that are performed on a given input. The output of one operation can be used as the input for the next, allowing for complex, multi-step processes to be created and executed easily.\n",
    "\n",
    "**Memory Functionality:** LangChain supports the concept of a memory for chain objects, allowing data to persist across multiple calls. This makes the chain a stateful object, which can be useful for certain types of applications.\n",
    "\n",
    "**Customizable Chains:** While LangChain provides many predefined chains, developers also have the flexibility to create custom chains tailored to their specific needs. This can be done by subclassing the Chain class and implementing the required methods.\n",
    "\n",
    "**Debugging and Verbose Mode:** Debugging in LangChain can be facilitated by setting the verbose mode to true, which prints out the internal states of the Chain object during its execution. This can help developers understand what's happening at each step and identify any issues.\n",
    "\n",
    "Overall, LangChain is a powerful and versatile framework for developers looking to build applications that leverage the power of large language models\n",
    "\n",
    "### Why we use Langchain ?\n",
    "\n",
    "Langchain is a python library that helps you interact with LLMs like ChatGPT and connect it external data and apps.\n",
    "\n",
    "In this lesson we will learn how to interact with ChatGPT to generate text. Many apps which are valued millions of dollars like JasperAI, CopyAI etc. do the same as part of their core business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To0cTjhBFT0V"
   },
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVyqccumI333",
    "outputId": "97613191-5b8f-49e0-9a29-043b7fda7683"
   },
   "outputs": [],
   "source": [
    "# Let's install necessary libraries\n",
    "\n",
    "#!pip install langchain\n",
    "#!pip install openai\n",
    "!pip install tiktoken\n",
    "!pip install python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cee7L_56FdMF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "my_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "print(f'openai api key is: {my_key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUNVYe0iORva"
   },
   "source": [
    "### Text Completion\n",
    "\n",
    "We will ask GPT to generate an outline for an article we wish to write\n",
    "\n",
    "As you can see from the output, you can generate a blog article outline from less than 5 lines of codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Co34TeW0ODRl",
    "outputId": "91495310-bcd1-45ee-dd21-488f76233a82"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "#text = \"Write me an outline on Tennis\"\n",
    "text = \"Hi how are you\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghdws72ROs9I"
   },
   "source": [
    "### Prompts and Prompt templates ✏️\n",
    "\n",
    "Before understanding the concept of prompt templates it is important that you must be aware about what does prompt actually means. Prompt is simply a textual instruction which we give to a model to provide some specific output.\n",
    "\n",
    "To better understand this let us assume that we want some outline about tennis. So for this our prompt could something be like \"Write me an outline on Tennis\". But what if we want to again want to get some outline about some other sport let say cricket. In such kind of scenarios the naive approach would be to simply rewrite the prompt with updated sport.\n",
    "\n",
    "But, if you are a computer science student you would be aware about the concept of code reproducibility and in the above mentioned naive approach this concept is getting violated as for every different city we are forced to rewrite the entire prompts with updated sport, so to make the process of creating the prompts efficient we use prompts templates.\n",
    "\n",
    "*Prompt templates are like ready-made templates which contains contextual information about the input parameter, where input parameter is simply the input provided by the end user. The below mentioned code snipped will help you understand how we can create prompts efficiently.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2-VN6LTGBIl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "#template = PromptTemplate.from_template(\"Write me an very very show outline on {input_parameter}?\")   \n",
    "#user_input = input(\"Enter sport : \")\n",
    "#prompt = template.format(input_parameter=user_input)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes baskeyball?\"\n",
    ")\n",
    "print(\"Prompt :\",prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "print(prompt)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "#product = \"Queen Size Sheet Set\"\n",
    "product = input(\"Please input a name of a product: \")\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT_DGdA9VGE8"
   },
   "source": [
    "## Chains\n",
    "\n",
    "Now we are going to use a Langchain concept Chains. Chains are responsible for the entire data flow inside Langchain. As we discussed above we are passing dynamic topic input variable to OpenAI. To accommodate this we will be using a chain called LLMChain. There are a bunch of chains supported in Langchain, we will talk about them later\n",
    "\n",
    "LLMChain takes the prompt from the prompt template we created above and fills it up with the dynamic input before passing to OpenAI LLM. Let's define LLMChain below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lszkNwMNVNiV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "print(\"Prompt :\",prompt)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "adjective = input('please input an adjective: ')\n",
    "print(adjective)\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "prompt = PromptTemplate(input_variables=[\"adjective\"], template=prompt_template)\n",
    "print(f'prompt is : {prompt}')\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNRq_YnoQByB"
   },
   "source": [
    "Now that we have created a prompt template and a chain we can now input any topic we want. Instead of topic \"Tennis\" we can input \"Cricket\" or any other topic of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUh0JGOgPxyv",
    "outputId": "cefee482-9ba6-4313-908b-555d2c511162"
   },
   "outputs": [],
   "source": [
    "print(chain.run(topic=\"Cricket\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVBNjz2OSzrs"
   },
   "source": [
    "Now let's extend it for a multi-input prompt. Let's generate an introductory paragraph to a blog post with variables title, audience and tone of voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqVEYaPPGJuM"
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"audience\", \"tone\"],\n",
    "    template=\"\"\"This program will generate an introductory paragraph to a blog post given a blog title, audience, and tone of voice\n",
    "\n",
    "    Blog Title: {title}\n",
    "    Audience: {audience}\n",
    "    Tone of Voice: {tone}\"\"\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJgvUP0pSVat",
    "outputId": "56838235-2e98-4580-9301-6b0808f89a5e"
   },
   "outputs": [],
   "source": [
    "print(chain.run(title=\"Best Activities in Toronto\", audience=\"Millenials\", tone=\"Lighthearted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP3dAgTgWq-r"
   },
   "source": [
    "## Combining Chains\n",
    "\n",
    "Often we would want to do multiple tasks using GPT. For example if we wish to generate an outline for a topic and use that outline to write a blog article we need to take the outline created from the first step and copy paste and paste as input to the second step\n",
    "\n",
    "Instead we can combine chains to achieve this in a single step. We will do this using a different type of chain called Sequential Chain. A sequential chain takes the output from one chain and passes on to the next. We will cover chains in more detail later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqHNoQ1OXLCC"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write me an outline on {topic}\",\n",
    ")\n",
    "llm = OpenAI(temperature=0.9, max_tokens=-1)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"\"\"Write a blog article in the format of the given outline \n",
    "\n",
    "    Outline:\n",
    "    {outline}\"\"\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIieZmmAXaCC",
    "outputId": "ff62bf70-503d-41b0-82a5-ea54a80a0e1d"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "catchphrase = overall_chain.run(\"Tennis\")\n",
    "print(catchphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
